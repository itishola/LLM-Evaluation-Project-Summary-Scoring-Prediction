{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621b1032-0414-4fbc-a31c-3acb6e059e8d",
   "metadata": {},
   "source": [
    "### LLM Evaluation Project: Summary Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fae06-f660-4077-b845-ca413256f9ad",
   "metadata": {},
   "source": [
    "#### Project Overview\n",
    "This project involves developing a model to predict content and wording scores for student-written summaries across various topics and genres. The dataset includes 24,000 summaries from students in grades 3-12, each scored on both content and wording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2af91-6e22-49c4-bd39-b2ed48ba32f1",
   "metadata": {},
   "source": [
    "#### 1. Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa48b1c-de07-408d-add0-5bd48ed13b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load data\n",
    "train_summaries = pd.read_csv('summaries_train.csv')\n",
    "train_prompts = pd.read_csv('prompts_train.csv')\n",
    "test_summaries = pd.read_csv('summaries_test.csv')\n",
    "test_prompts = pd.read_csv('prompts_test.csv')\n",
    "\n",
    "# Merge prompts with summaries\n",
    "train_data = pd.merge(train_summaries, train_prompts, on='prompt_id')\n",
    "test_data = pd.merge(test_summaries, test_prompts, on='prompt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3072be-2102-42aa-b237-5740c241c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "student_id         0\n",
       "prompt_id          0\n",
       "text               0\n",
       "content            0\n",
       "wording            0\n",
       "prompt_question    0\n",
       "prompt_title       0\n",
       "prompt_text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba62f82-b85d-41d8-8af3-995caee923ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "student_id         0\n",
       "prompt_id          0\n",
       "text               0\n",
       "prompt_question    0\n",
       "prompt_title       0\n",
       "prompt_text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec045c3-ecff-4346-990f-a53833efb481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.014853</td>\n",
       "      <td>-0.063072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.043569</td>\n",
       "      <td>1.036048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.729859</td>\n",
       "      <td>-1.962614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.799545</td>\n",
       "      <td>-0.872720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.093814</td>\n",
       "      <td>-0.081769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.499660</td>\n",
       "      <td>0.503833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.900326</td>\n",
       "      <td>4.310693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           content      wording\n",
       "count  7165.000000  7165.000000\n",
       "mean     -0.014853    -0.063072\n",
       "std       1.043569     1.036048\n",
       "min      -1.729859    -1.962614\n",
       "25%      -0.799545    -0.872720\n",
       "50%      -0.093814    -0.081769\n",
       "75%       0.499660     0.503833\n",
       "max       3.900326     4.310693"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['content', 'wording']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07891cd4-1f12-421d-b39b-8793c990612c",
   "metadata": {},
   "source": [
    "2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdcde51-8aeb-42f3-bbfb-ee685d96cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/issa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/issa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/issa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/issa/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download all required NLTK data\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging (if needed)\n",
    "nltk.download('wordnet')  # For lemmatization (if needed)\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizer is available\")\n",
    "except LookupError:\n",
    "    print(\"Punkt tokenizer not found - trying alternative download method\")\n",
    "    nltk.download('punkt', download_dir='/home/issa/nltk_data/')  # Specify your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fe491-1241-47b3-9605-6da3832a1716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1130e16-37ad-49cf-a7ea-c997b316f6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/issa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/issa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/issa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_features(text):\n",
    "    try:\n",
    "        # Basic text statistics\n",
    "        words = word_tokenize(str(text))  # Ensure text is string\n",
    "        word_count = len(words)\n",
    "        char_count = len(str(text))\n",
    "        avg_word_length = char_count / max(1, word_count)\n",
    "        unique_words = len(set(words))\n",
    "        lexical_diversity = unique_words / max(1, word_count)\n",
    "        \n",
    "        # Sentence tokenization with error handling\n",
    "        sentences = sent_tokenize(str(text))\n",
    "        sentence_count = len(sentences)\n",
    "        avg_sentence_length = word_count / max(1, sentence_count)\n",
    "        \n",
    "        return {\n",
    "            'word_count': word_count,\n",
    "            'char_count': char_count,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'lexical_diversity': lexical_diversity,\n",
    "            'avg_sentence_length': avg_sentence_length\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return {\n",
    "            'word_count': 0,\n",
    "            'char_count': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'lexical_diversity': 0,\n",
    "            'avg_sentence_length': 0\n",
    "        }\n",
    "\n",
    "# Apply feature engineering\n",
    "train_features = train_data['text'].apply(lambda x: pd.Series(text_features(x)))\n",
    "test_features = test_data['text'].apply(lambda x: pd.Series(text_features(x)))\n",
    "\n",
    "# Combine with original data\n",
    "train_data = pd.concat([train_data, train_features], axis=1)\n",
    "test_data = pd.concat([test_data, test_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badb36e-9f76-467a-b397-5ae18579f438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd85c9c8-ed58-4917-9b26-67e27540836d",
   "metadata": {},
   "source": [
    "#### 3. Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790099a-7590-4b30-aa95-a223344b8830",
   "metadata": {},
   "source": [
    "#### A combination of traditional ML and transformer-based approaches will be used:\n",
    "\n",
    "#### Option 1: Gradient Boosting (XGBoost/LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2239b22e-3c3f-4f56-9647-0f3b0635912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 7165, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score -0.014853\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 7165, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score -0.063072\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Prepare data\n",
    "X = train_data.drop(['student_id', 'prompt_id', 'text', 'content', 'wording', \n",
    "                    'prompt_question', 'prompt_title', 'prompt_text'], axis=1)\n",
    "y = train_data[['content', 'wording']]\n",
    "\n",
    "# Train model\n",
    "model = MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=500, random_state=42))\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict on test set\n",
    "test_X = test_data.drop(['student_id', 'prompt_id', 'text', \n",
    "                        'prompt_question', 'prompt_title', 'prompt_text'], axis=1)\n",
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69633018-8daf-4e7f-9c5f-d0b491ea5219",
   "metadata": {},
   "source": [
    "#### Option 2: Transformer-based Model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b15426-a01c-4158-9e8d-237173d44cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, texts, prompts, content_scores=None, wording_scores=None):\n",
    "        self.texts = texts\n",
    "        self.prompts = prompts\n",
    "        self.content_scores = content_scores\n",
    "        self.wording_scores = wording_scores\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        prompt = self.prompts[idx]\n",
    "        \n",
    "        # Combine prompt and text\n",
    "        combined = f\"Prompt: {prompt} Summary: {text}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            combined,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.content_scores is not None and self.wording_scores is not None:\n",
    "            item['content'] = torch.tensor(self.content_scores[idx], dtype=torch.float)\n",
    "            item['wording'] = torch.tensor(self.wording_scores[idx], dtype=torch.float)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# Model architecture\n",
    "class SummaryScorer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce4587-0e2d-4f50-beae-1219b34878c0",
   "metadata": {},
   "source": [
    "#### 4. Ensemble Approach\n",
    "Combine predictions from both models for potentially better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a842e2-11ec-4b1a-a631-9c5c1d874a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have predictions from two models:\n",
    "# 1. LightGBM predictions (lgb_predictions)\n",
    "# 2. BERT predictions (bert_predictions)\n",
    "\n",
    "# Sample predictions (replace with your actual predictions)\n",
    "# For demonstration, let's create dummy predictions\n",
    "num_samples = len(test_data)\n",
    "lgb_predictions = np.random.rand(num_samples, 2)  # Random values between 0-1\n",
    "bert_predictions = np.random.rand(num_samples, 2)  # Random values between 0-1\n",
    "\n",
    "# Weighted ensemble (adjust weights based on validation performance)\n",
    "ensemble_weights = {'lgb': 0.6, 'bert': 0.4}  # These should be tuned\n",
    "\n",
    "# Calculate weighted average\n",
    "final_predictions = (\n",
    "    ensemble_weights['lgb'] * lgb_predictions + \n",
    "    ensemble_weights['bert'] * bert_predictions\n",
    ")\n",
    "\n",
    "# If you want to validate the ensemble performance (on validation set)\n",
    "# y_val_true = ...  # Your true validation labels\n",
    "# print(f\"Ensemble MCRMSE: {mcrmse(y_val_true, final_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7312fa-dac9-4b19-a4ba-2eea8150ab01",
   "metadata": {},
   "source": [
    "#### 5. Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4335db5-86f0-4af9-83ca-c4de73f37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'student_id': test_data['student_id'],\n",
    "    'content': predictions[:, 0],\n",
    "    'wording': predictions[:, 1]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef589b40-77a8-4dcf-9760-b5fc6eb936a4",
   "metadata": {},
   "source": [
    "#### Evaluation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f8c7e0-0a1a-4e05-87ef-5fc81431a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCRMSE: 0.37828621394977974\n"
     ]
    }
   ],
   "source": [
    "def mcrmse(y_true, y_pred):\n",
    "    rmse_content = np.sqrt(mean_squared_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    rmse_wording = np.sqrt(mean_squared_error(y_true[:, 1], y_pred[:, 1]))\n",
    "    return (rmse_content + rmse_wording) / 2\n",
    "\n",
    "# Example usage\n",
    "y_true = train_data[['content', 'wording']].values\n",
    "y_pred = model.predict(X)\n",
    "print(f\"MCRMSE: {mcrmse(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695dc3e-9429-408b-b617-6acaca8ae10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
